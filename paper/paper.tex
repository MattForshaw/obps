\documentclass[sigconf]{acmart}
\setcopyright{rightsretained}

\usepackage{listings}
\usepackage{booktabs}
\usepackage{url}
\usepackage[utf8]{inputenc}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{float}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bbm}
\usepackage[scientific-notation=true]{siunitx}

\usepackage{amssymb}
\usepackage{tabulary}

\usepackage{etoolbox}

\usepackage{algorithm}
\usepackage{algorithmic}

\makeatletter
\newcommand\fs@norules{\def\@fs@cfont{\bfseries}\let\@fs@capt\floatc@ruled
  \def\@fs@pre{}%
  \def\@fs@post{}%
  \def\@fs@mid{\kern3pt}%
  \let\@fs@iftopcapt\iftrue}
\makeatother
\floatstyle{norules}
\restylefloat{algorithm}

\let\tinymatrix\smallmatrix
\let\endtinymatrix\endsmallmatrix
\patchcmd{\tinymatrix}{\scriptstyle}{\scriptscriptstyle}{}{}
\patchcmd{\tinymatrix}{\scriptstyle}{\scriptscriptstyle}{}{}
\patchcmd{\tinymatrix}{\vcenter}{\vtop}{}{}
\patchcmd{\tinymatrix}{\bgroup}{\bgroup\scriptsize}{}{}

\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}

\usepackage[draft,index]{fixme}
\fxsetup{theme=color,mode=multiuser,layout=inline,draft}

% correct bad hyphenation here
%\hyphenation{op-tical net-works semi-conduc-tor}

\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Online (Bandit) Policy Selection for EASY-Backfilling}

\author{Eric Gaussier}
\affiliation{%
  \institution{Univ. Grenoble Alpes, CNRS, Grenoble INP, LIG}
  \country{France}}
\email{eric.gaussier@imag.fr}
\author{J\'er\^ome Lelong}
\affiliation{%
  \institution{Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK}
  \country{France}}
\email{jerome.lelong@imag.fr}
\author{Valentin Reis}
\affiliation{%
  \institution{Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LIG}
  \country{France}}
\email{valentin.reis@imag.fr}
\author{Denis Trystram}
\affiliation{%
  \institution{Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LIG}
  \country{France}}
\email{denis.trystram@imag.fr}

\begin{abstract}

  The EASY-FCFS heuristic is the basic building block of job scheduling
  policies in most parallel High Performance Computing (HPC) platforms. Despite
  its good properties (simplicity, no starvation), it could still be improved
  on a per-system basis. This tuning process is difficult because of
  non-linearities in the scheduling process. The study proposed here considers
  an online approach to the automatic tuning of the EASY heuristic for HPC
  platforms. More precisely, we consider the problem of selecting a reordering
  policy of the job queue under several feedback modes. We show via a
  comprehensive experimental campaign that noisy feedback (using a weak
  simulator) recovers existing in-hindsight results that allow to divide the
  average waiting time up to a factor of 2. Moreover, we show that bandit
  feedback can be used by a simple multi-armed bandit algorithm to decrease the
  average waiting time down to 40\% of its original value without using a
  simulator.

\end{abstract}

\maketitle

\section{Introduction}
\label{sec:intro}

Plan :
\begin{itemize}
\item Context - message : les machines sont de + en + complexes, les systemes
de management restent rudimentaires face a la complexite croissante de
l'architecture et des applications.
\item les habitudes et usages existants : justification du FCFS-EASY-BF,
developper rapidement ce qui existe
\item ce que l'on propose
\end{itemize}


Providing the computing infrastuctures needed to solve actual complex problems
arising in the various fields of the modern society (including climate change,
health, green energy or security) is a strategic challenge.
In particular, High Performance Computing (HPC) systems are evolving to
extreme-scale parallel and distributed platforms.
The race for always more computing power and storage capacity does not only
lead to sophisticated specific exascale platforms, but the objective of the
community is to design efficient sustained Petascale platforms.
However, there is still a long way to go and many scientific and technical
problems to solve by the system management software in order to adapt to such
large-scale evolution and the increasing complexity
in both architecture and applications.
The existing job and resource management sofwares allow to run tens thousands
jobs on hundreds thousands cores.
They are based on robust and rather simple policies.

\bigskip
The management generates a huge amount of data.
During the last few years, there was an explosion of the number of of works at
the interface of HPC and bigdata, dealing with learning algorithms
(including the authors of this paper~\cite{learningruntimes}).
Most of these studies target the process to determine more or less accurately
the value of some specific key parameters, with the idea of
learning better estimates should improve the performances of the resource
manager.

\bigskip
Our idea within this work is to learn how the management system behave within
the classical framework of of selecting jobs of the submission queue under
EASY-backfilling.
Contrarily to most existing studies which consider only one execution trace, we
extend the learning on a multiple trace basis.
Indeed, learning for a specific trace leads to limited results, customized for
one trace on a given platform (what happens if we target another platform?).
Moreover, this is usually inefficient to deal with extreme unpredictable
events.

\subsection{Contributions}

\section{Related Work}
\label{sec:rw}

This section presents related works in the area of scheduling for batch scheduling.

\subsection{Scheduling heuristics in HPC platforms}

While parallel job scheduling is a well studied theoretical
problem~\cite{leung2004handbook}, the practical ramifications, varying
hypotheses, and inherent uncertainty of the problem in HPC have driven
practitioners and researchers alike to use and study simple heuristics.
The two most popular heuristics for HPC platforms are EASY~\cite{easy} and
Conservative~\cite{Mu'alem:2001:UPW:380314.380315} Backfilling.

While Conservative Backfilling offers many advantages~\cite{bfchar}, it has a
significant computational overhead, perhaps explaining why most of the machines
of the top500 ranking~\cite{top500} still use at the time of this publication a
variant of EASY Backfilling.

\subsection{EASY}
There is a large body of work seeking to improve EASY. Indeed, while the
heuristic is used by various resource and job management softwares (most notably
SLURM~\cite{SLURMdocSCHED}), this is rarely done without fine tunings by system
administrators.

Several works explore how to tune EASY by reordering waiting
and/or backfilling queues~\cite{Tsafrir_easypp_2005}, sometimes even in a
randomized manner~\cite{1592720}, as well as some
implementations~\cite{Jackson2001}. However, as successful as they may be, these works
do not address the dependency~\cite{variability} of scheduling metrics on the
workload. Indeed these studies most often report \textit{post-hoc} performance since
they compare algorithms after the workload is known.

The dynP scheduler~\cite{streit_selftuning_2002} proposes a systematic
method to tuning these queues, although it requires simulated scheduling runs
at decision time and therefore costs much more than the natural execution of EASY.

\subsection{Data-aware resource management}

There is a recent focus on leveraging the high amount of data available in
large scale computing systems in order to improve their behavior. Some works
use collaborative filtering to colocate tasks in clouds by estimating
application interference~\cite{7516031}.  Others are closer to the application
level and use binary classification to distinguish benign memory faults from
application errors in order to execute recovery algorithms (see
~\cite{fmodeling} for instance).


Several works take a similar approach in the context of HPC, in
particular~\cite{Tsafrir_easypp_2005,learningruntimes}, hoping that better job
runtime estimations should improve the scheduling~\cite{chiang_impact_2002}.
Some algorithms estimate runtime distributions model and choose jobs using
probabilistic integration procedures~\cite{Nissimov2008}.

However, these works do not address the duality between the cumulative and
maximal scheduling costs, as mentionned in~\cite{learningruntimes}.

A recent work~\cite{mlmem} proposes to help
users specify better memory requirements by applying supervised learning
techniques to job metadata.

While these previous works intend to estimate uncertain parameters, they lack
in validating the impact on the system. We consider in this paper a more
pragmatic approach, which is to directly learn a scheduling policy from a given
seach space.

\subsection{Multi-Armed Bandits}

A multi-armed bandit (MAB) problem is a sequential allocation problem with
partially observable rewards. At every round, an action (an arm) must be chosen
in a fixed set and the corresponding reward is observed. The goal of a MAB
algorithm is to maximize the total reward obtained in a number of rounds.
There are a number of works that address this problem under a variety of
constraints, The two most popular settings are the original stochastic  case,
and the adversarial case. See ~\cite{thompson} for the original work on the
stochastic case and ~\cite{Auer2002} for the UCB family of algorithms. The work
of ~\cite{Banos} is the earliest known work to us for the adversarial case
see~\cite{nonstoch} for the EXP family of algorithms. We refer to the
review~\cite{bubnow} for a comprehensive overview of the field. While these
algorithms bound the cumulative difference in loss to the best arm (the
regret), they have functional constraits such as the fact that the rewards
should be contained in a range. The\cite{Auer2002} heuristic called
Epsilon-Greedy is known to achieve good results in practice, and does not have
this requirement.

\section{Problem setting}
\label{sec:problem_setting}

This section presents the systems under study and the scheduling problem.  It
first introduces the EASY-Backfilling heuristic and gives
\textcolor{orange}{(fix ugly sentence)} the problem statement.

\subsection{System Description}
\label{sub:sysdesc}

The problem addressed in this work is the core logic of Resource and Job
Management Systems (RJMS) such as SLURM~\cite{SLURMdocSCHED}, PBS~\cite{PBSdoc},
OAR~\cite{capit2005batch}, Cobalt~\cite{Cobalt}, and more recently Flux~\cite{flux2014}.

The crucial part of batch scheduling software is the scheduling algorithm that determines
where and when the submitted jobs are executed. The process is as follows: jobs
are submitted by end-users and queued until the scheduler selects one of them
for running. Each job has a provided bound on the execution time and some
resource requirements (number and type of processing units). Then, the RJMS
drives the search for the resources required to execute this job. Finally, the
tasks of the job are assigned to the chosen nodes.

In the classical case, these softwares need to execute a set of concurrent
parallel jobs with rigid (known and fixed) resource requirements on a HPC
platform represented by a pool of $m$ identical resources. This is an on-line
problem since the jobs are submitted over time and their characteristics are only known when they are released.
Below is the description and the notations of the characteriscs of job $j$:

\begin{itemize}
  \item Submission date $r_j$ (also called \textit{release date})

  \item Resource requirement $q_j$ (number of processors)

  \item Actual running time $p_j$ (sometimes called \textit{processing time})

  \item Requested running time $\widetilde{p_j}$ (sometimes called \textit{walltime}), which is an upper bound of $p_j$.

\end{itemize}

The resource requirement $q_j$ of job $j$ is known when the job is submitted at
time $r_j$, while the requested running time $\widetilde{p_j}$ is given by the
user as an estimate. Its actual value $p_j$ is only known \textit{a posteriori}
when the job really completes.  Moreover, the users have incentive to
over-estimate the actual values, since jobs may be ``killed'' if they
surpass the provided value.
\subsection{EASY Backfilling}
\label{sub:easy}

The selection of the job to run is performed according to a
scheduling policy that establishes the order in which the jobs are executed.
EASY-Backfilling is the most widely used policy due to its simple and robust
implementation and known benefits such as high system
utilization~\cite{easy}. This strategy has no worst case guarantee beyond
the absence of starvation (i.e. every job will be scheduled at some moment).

The EASY-FCFS heuristic uses a job queue to select and backfill jobs.  At any
time that requires a scheduling decision (i.e. job submission or termination),
the scheduler goes through the job queue in First-Come,First-Serve (FCFS) order
and starts jobs until it finds a job that can not be started right away. It
then makes a reservation for this job at the earliest predictable time and
starts \textit{backfilling} the job queue in FCFS order, starting any job that
does not delay the unique reservation.

\subsection{Scheduling Objective}
\label{sub:scheduling_objectives}

A system administrator may use one or multiple cost metric(s). Our study of
scheduling performance relies on the waiting times of the jobs, which is one of
the more commonly used objectives.

    \begin{equation}
      \textbf{Wait}_j =  start_j-r_j
    \end{equation}

Like other cost metrics, the waiting time is usually considered in its
\textit{cumulative} version, which means that one seeks to minimize the average
waiting time (\textbf{AvgWait}). It is worth noting that the \textbf{MaxWait},
a.k.a the maximal value of the waiting time of all the jobs is also worthy of
interest. Unfortunately, these criteria can sometimes be dual in practice,
making the problem bi-objective. Subsection~\ref{sub:th} will outline our
approach to managing this aspect.

\subsection{Problem Statement}

Our problem statement is: \textbf{How to tune EASY-Backfilling in an online manner?}
\textcolor{orange}{introduce the difference in workloads, the duality of avg and max, and the online problem}

\section{Tuning EASY by reordering and thresholding the job queue}
\label{sec:framework}

This section presents two mechanisms for safely tuning the EASY-Backfilling:
job queue reordering and job thresholding. Together, these two building blocks
constitute a robust framework for tuning EASY.

\subsection{Reordering the job Queue}
\label{subsec:policies}

The EASY heuristic uses a job queue to select and backfill jobs. While this job
queue is ordered in FCFS order in the original heuristic, it is possible to
reorder it at will. We settle on a reasonable search space of 10 reordering policies.

%wait mwait lpf spf sqf lqf expfact mexpfact lrf srf laf saf
\begin{enumerate}
  \item FCFS: First-Come First-Serve, which is the widely used default policy~\cite{easy}.
  \item LCFS: Last-Come First-Serve.
  %\item SPF: Smallest estimated Processing time $\widetilde{p_{j}}$ First ~\cite{bfchar}.
  \item LPF: Longest estimated Processing time First.
  \item LQF: Largest resource requirement $q_j$ First.
  %\item SQF: Smallest resource requirement First.
  \item LEXP: Largest Expansion Factor First~\cite{bfchar}, where the expansion
    factor is defined as follows:
  \begin{equation} \frac{start_j - r_j + \widetilde{p_j}}{\widetilde{p_j}} \end{equation}
  where $start_j$ is the starting time of job $j$.
  \item SEXP: Smallest Expansion Factor First
  \item LRF: Largest Ratio $\frac{p_j}{q_j}$ First
  \item SRF: Smallest Ratio First
  \item LAF: Largest Area $ p_j \times q_j$ First
  %\item SAF: Smallest Area First
\end{enumerate}

This search space is designed with the goal of being as semantically diverse as
possible without making any judgement on which policy should perform well in
practice. In the following, we denote these policies by $P_i$ with $i = 1
\ldots 10$.

\subsection{Thresholding}
\label{sub:th}
As existing works point out, reordering the job queue means losing the
no-starvation guarantee and some individual jobs therefore can wait an undue
amount of time. It is possible to introduce a thresholding mechanism in order
to prevent this behavior: When a job's \textit{waiting time so far} exceeds a
fixed threshold $\Theta$, it is jumped at the head of the queue. We denote by
EASY($P,\Theta$) the scheduling policy that starts and backfill jobs according to
the (thresholded) reordering policy $P$. For the sake of completeness,
Algorithm~\ref{alg:EASY} describes the EASY($P,\Theta$) heuristic.

\begin{algorithm}[h]
  \caption{EASY($P,\Theta$) policy}
  \begin{algorithmic}[1]
    \renewcommand{\algorithmicrequire}{\textbf{Input:}}
    \renewcommand{\algorithmicensure}{\textbf{Output:}}
    \REQUIRE Queue $Q$ of waiting jobs.
    \ENSURE None (calls to $Start()$)
    \STATE Sort $Q$ according to $P_R$
    \STATE Move all jobs of $Q$ for which $wait_j > \Theta$ ahead
    of the queue (breaking ties in FCFS order).
    \\ \textit{Starting jobs until the machine is full}
    \FOR {job $j$ in Q do}
    \IF {$j$ can be started given the current system use.}
    \STATE Pop $j$ from Q
    \STATE $Start(j)$
    \ELSE
    \STATE Reserve $j$ at the earliest
    time possible according to the estimated running times
    of the currently running jobs.
    \\ \textit{Backfilling jobs}
    \FOR {job $j'$ in $Q\setminus\{j\}$}
    \IF {$j'$ can be started without delaying the reservation on $j$.}
    \STATE Pop $j'$ from $Q$
    \STATE $Start(j')$
    \ENDIF
    \ENDFOR
    \STATE \textbf{break}
    \ENDIF
    \ENDFOR
  \end{algorithmic}
  \label{alg:EASY}
\end{algorithm}

\section{Online tuning}
\label{sec:online}

\begin{figure*}[ht]
  \centering
  \includegraphics[scale=0.6]{figures/variability.pdf}
  \caption{Variability in the weekly average waiting time in the KTH-SP2 trace (see Subsection~\ref{sub:traces}) for 7 different policies.}
  \label{fig:mosn}
\end{figure*}

We present here the strategies we have retained for selecting a policy. We will refer to the period during which a selected policy is applied as the \textit{policy period} and will denote the length of this period as $\Delta$. The time interval is thus divided into periods of equal lengths ($\Delta$): $\Delta_0, \cdots, \Delta_T$, where $T$ is the index of the current policy period. A new policy is selected at the beginning of each period and applied during the whole period.

We further assume that there is a certain regularity among periods, \textit{i.e.} that the distributions underlying the jobs submitted do not radically differ between consecutive periods. This assumption is validated in the study presented in~\ref{???}. It further entails that the behavior of a policy on the previous periods reflects its behavior on the current one, so that the selection of a policy can be based on its behavior on previous periods. However, there may be some variability between different periods for certain cost metrics. This is illustrated in Figure~\ref{fig:mosn} that displays the average waiting time for a trace using weekly periods encompassing one year. As one can note, the average waiting time varies a lot from one week to the other, for all the 7 policies considered. This indicates that when when the cost metric is averaged over different periods, there is a tradeoff to find in between longer periods that would somehow limit the variability, and shorter ones that yield more values for the estimation. We will come back to this issue below.

The selection of a policy is of course reminiscent of reinforcement learning. It is important to note, however, that a pure reinforcement learning approach is difficult to develop in our context as \textcolor{orange}{???}. We rely here on simpler, yet we believe more effective, strategies to solve this problem. These strategies are applied online and rely on exact simulation, noisy simulation and $\epsilon$-greedy bandit exploration.

\subsection{Policy selection with exact simulation}
\label{sub:feedback}

Several simulators have been developed for "playing" reordering policies on a given set of jobs. We rely in this study on a lightweight simulator (see Section~\ref{sec:experiments}) that can efficiently simulate different policies. Such simulators are interesting inasmuch as they provide an estimate of the cost of a given policy on a set of jobs, as described below.

Let $l(\Delta_t)$ denote the number of jobs \textit{submitted} during the period $\Delta_t$ ($0 \le t < T$), and $P_i$ ($1 \le i \le 10$) one reordering policy (defined in Section~\ref{subsec:policies}). The cost of policy $P_i$ during the period $\Delta_t$ is defined as:
%
\begin{equation}
\label{eq:cost-exact}
w_{\mbox{exact}}(t;P_i) = \sum_{j=1}^{l(\Delta_t)} \mbox{Wait}_j^{P_i}
\end{equation}
%
where $\mbox{Wait}_j^{P_i}$ denotes the estimate provided by the simulator of the waiting time for job $j$ according to policy $P_i$. The estimation of the cost of a policy over all the periods preceding the current period can then be defined as:
%
\begin{equation}
\label{eq:tot-cost-exact}
w(\rightarrow T;P_i) = \sum_{t=0}^{T-1} \lambda^{T-1-t} w_{\mbox{exact}}(t;P_i)
\end{equation}
%
$\lambda \in [0,1]$ is a decay parameter that can be used to privilege recent history (\textit{i.e.} recent periods). One then selects the policy $P$ that minimizes the above cost for the period $\Delta_T$:
%
\begin{equation}
\label{eq:select-exact}
P = \mbox{argmin}_{P_i, 1 \le i \le 10} w(\rightarrow T;P_i)
\end{equation}

The above cost directly corresponds to the cumulative waiting time of the policy over the preceding periods (more precisely to the cumulative simulated estimate of the waiting time of the policy over the preceding periods) so that the length of the period has no impact here.

Furthermore, in the context of this study and as detailed in Section~\ref{sec:experiments}, we rely on averages over several execution traces in order to obtain reliable estimate of the behavior of different policies and policy selection strategies. Such traces are typically obtained by simulation. Using the same simulator for generating the traces and estimating the cost as defined in Eqs \ref{eq:cost-exact} and \ref{eq:tot-cost-exact} would however be too optimistic and would represent an upper bound on what can be achieved by a selection strategy based on simulation. In order to have a more realistic estimate of the behavior of a selection strategy based on simulation, we introduce noise in the simulator, as described below.

\subsection{Policy selection with noisy simulation}
\label{sub:noisy}

In order to simulate how the simulation strategy for selecting policies would work on non simulated traces, we randomly introduce noise in the estimate of the waiting time defined by Eq.~\ref{eq:cost-exact} by rescaling it, either down or up, for each job:
%
\begin{enumerate}
\item For each job $j$ submitted during $\Delta_t$: $\rho_j \sim U[0.85,1.15]$
\item Then:
%
\begin{equation}
\label{eq:cost-noisy}
w_{\mbox{noisy}}(t;P_i) = \sum_{j=1}^{l(\Delta_t)} \rho_j \, \mbox{Wait}_j^{P_i}
\end{equation}
%
\end{enumerate}
%
$\rho$ is uniformly sampled in the interval $[0.85,1.15]$, thus adding a +/-15\% noise on the waiting time estimated by the simulator. The overall cost is then defined in the same way as before, leading to:
%
\begin{equation}
\label{eq:tot-cost-noisy}
w(\rightarrow T;P_i) = \sum_{t=0}^{T-1} \lambda^{T-1-t} w_{\mbox{noisy}}(t;P_i)
\end{equation}
%
As before (see Eq.~\ref{eq:select-exact}), the policy that minimizes the above cost is selected for the period $\Delta_T$.

Instead of introducing noise in the output of the simulator when selecting the policy, one could have introduced it when generating the execution traces. This would however have prevented us from comparing the exact and noisy selection methods, the first one representing an upper bound on the second one. As we will see in Section~\ref{sec:experiments}, there is little difference between the two selection methods, which is an important result of our study.

Lastly, as before, the length of the period has no impact here as the cost metric corresponds to the cumulative simulated waiting time for each policy.

\subsection{Policy selection with $\epsilon$-greedy exploration}
\label{sub:bandit}

The previous strategies estimate the cost of all policies over all previous periods; the best policy according to this cost is then selected for the current period, the costs of all policies being updated for the next period. This is interesting as one maintains a complete knowledge of all policies over time. However, this requires computing many estimates at each period (as many as there are policies), which can be time consuming even with lightweight simulators.

We thus explore here a more efficient strategy that dispenses with estimating the cost of all policies at a given time and only updates the cost of the policy that is currently being used. This strategy makes use of the $\epsilon$-greedy exploration method, standard in reinforcement learning \cite{Watkins:1989}, to tradeoff exploitation and exploration, and relies, as before, on past estimates of the cost to select the best policy in the exploitation mode.

Let $l(\Delta_t)$ denote this time the number of jobs \textit{finished} during the period $\Delta_t$. We define the cost of policy $P_i$ during the period $\Delta_t$ as:
%
\[
w_{\epsilon}(t;P_i) =
\left\{
\begin{array}{ll}
 \sum_{j=1}^{l(\Delta_t)} \textbf{Wait}_j^{P_i} & \mbox{if} \, P_i \, \mbox{used during} \, \Delta_t \nonumber\\
 0 & \mbox{otherwise}
 \end{array}
\right.
\]
%
As one can note, the actual waiting time is used here, so that one dispenses with the use of a simulator for efficiency considerations. This indeed leads to faster and more accurate estimate of the cumulative waiting time, however only for the policy that is used during $\Delta_t$. This also explains the difference between the definitions of $l(\Delta_t)$: in the previous selection strategies, one relies on a simulated estimate so that jobs need not be finished during $\Delta_t$, but only submitted; in contrast, to know their actual waiting time, jobs need be finished during $\Delta_t$.

The cost over all previous period of a policy $P_i$ is then defined as:
%
\begin{equation}
\label{eq:tot-cost-greedy}
w(\rightarrow T;P_i) = \frac{1}{\sum_{t=0}^{T-1} \mathbbm{1}(P_i,t) l(\Delta_t)} \sum_{t=0}^{T-1} \mathbbm{1}(P_i,t) \lambda^{T-1-t} w_{\epsilon}(t;P_i)
\end{equation}
%
where $\mathbbm{1}(P_i,t) = 1$ if $P_i$ is the policy used during the period $\Delta_t$ and $0$ otherwise. The normalization by $\sum_{t=0}^{T-1} \mathbbm{1}(P_i,t) l(\Delta_t)$ is necessary here to ensure that policies remain comparable over time, as not normalizing would favor the policy that was first selected, even if this choice was random. This normalization however entails that the size of the policy period is important: it should not be too large so as to avoid relying on too few points for estimating the cost, and it should not be too small to avoid extreme variations between periods. As explained in Section~\ref{sec:experiments}, we rely in this study on periods of one day and one week.

The selection of the policy to be used during the current time period $\Delta_T$ is then based on the standard $\epsilon$-greedy exploration strategy:
%
\begin{itemize}
\item With probability $(1-\epsilon)$, select the policy $P$ that minimizes the cost over previous time periods (exploitation mode):
%
\begin{equation}
\label{eq:select-greedy}
P = \mbox{argmin}_{P_i, 1 \le i \le 10} w(\rightarrow T;P_i)
\end{equation}
%
\item With probability $\epsilon$, select a policy $P_i, \, 1 \le i \le 10$, at random (exploration mode).
\end{itemize}

Several studies propose to update $\epsilon$ over time, the exploration being less important once the estimates for the different policies are reliable \cite{Tokic:2010}. This strategy has however not been beneficial in our case. We believe this is due to the stochastic nature of the traces we use, as explained in Section~\ref{sec:experiments}.

%\begin{algorithm}[h]
%  \caption{Epsilon-Greedy Bandit policy}
%  \begin{algorithmic}[1]
%    \renewcommand{\algorithmicrequire}{\textbf{Input:}}
%    \renewcommand{\algorithmicensure}{\textbf{Output:}}
%    \REQUIRE $0 < \epsilon < 1$, $K>1$
%    \FOR {$t = 1, 2, \ldots$}
%    \STATE Let $i_t$ be the reordering heuristic with the highest current
%    average reward.
%    \STATE With probability $1-\epsilon$ use $i_t$ and with probability
%    $\epsilon$ use a random reordering heuristic.
%    \ENDFOR
%  \end{algorithmic}
%  \label{alg:epsg}
%\end{algorithm}

\section{Experiments}
\label{sec:experiments}

This section compares the different approaches to policy selection via a
comprehensive experimental campaign.  Subsection~\ref{sub:protocol} outlines
the experimental protocol used and Subsection~\ref{sub:results} contains the
experimental results and their discussion.

\subsection{Experimental Protocol}
\label{sub:protocol}

The Evaluation of computer systems performance is an intricate
task\cite{feitbook}. Here, the two main difficulties are choosing a simulation
model and taking into account the variability in the average waiting times.
This section presents our approach and provides a replicable workflow
for the interested reader.

\subsubsection{Traces}
\label{sub:traces}

\begin{table}[ht]
  \centering
  \ra{1.3}
  \caption{Workload logs used in the simulations.}
  \label{tab:logs}
  \begin{tabular}{@{}lrrrr@{}}
    \hline
    Name          & Year & \# MaxProcs & \# Jobs & Duration\\
    \hline
    KTH-SP2       & 1996 & 100         & 28k     & 11 Months\\
    CTC-SP2       & 1996 & 338         & 77k     & 11 Months\\
    SDSC-SP2      & 2000 & 128         & 59k     & 24 Months\\
    SDSC-BLUE     & 2003 & 1,152       & 243k    & 32 Months\\
    ANL-Intrepid  & 2009 & 163840      & 68k     & 9  Months\\
    CEA-Curie     & 2012 & 80,640      & 312k    & 3  Months\\
    Unilu-Gaia    & 2014 & 2,004        & 51k     & 4  Months\\
    \hline
  \end{tabular}
\end{table}

The experimental campaign used in this work uses a mixture of small and large
systems from different periods. Table~\ref{tab:logs} presents the 7 logs, which
can all be obtained from the Parallel Workload
Archive~\cite{Feitelson20142967}. We use the 'cleaned'\footnote{\lstinline[basicstyle=\ttfamily\color{blue}]|http://www.cs.huji.ac.il/labs/parallel/workload/logs.html#clean|} version of the logs as
per the archive. We impose an additional
filtering step to the workloads\footnote{This filtering step is available in
the reproducible workflow\cite{repro} as shell script
\lstinline[basicstyle=\ttfamily\color{blue}]|misc/strong\_filter|} in order to
clean singular data. More precisely, we apply the following modifications:

\begin{enumerate}
  \item If the number of allocated or requested cores of a job  exceeds the size of the machine, we remove the job.
  \item If the number of allocated or requested cores is negative, we use the available positive value as the request. If both are negative, we remove the job.
  \item If the runtime or submission time is negative, we remove the job.
\end{enumerate}

\subsubsection{Simulator}

The choice of a simulator is a critical part of experimental validation that
raises a tradeoff between precision and runtime. High precision can be obtained
by carefully modeling the platform and its network topology, or extracting
information from the jobs from their sources or post-morted logs. This is the
approach used by high-fidelity batch scheduling simulators such as
Batsim~\cite{batsim}. In the present work, the focus is on studying the
EASY-Backfilling mechanism in itself, without adressing the allocation problem.
Moreover, the experimental protocol used here requires many simulation runs.
Therefore, we set the precision/runtime tradeoff at the point which minimizes
simulation runtimes. We discard all topological information relative to the
platform and use the job processing times of the original workloads. In this
setup, the processors are considered to be undistinguishable from each other,
and jobs can be discontinuously mapped to any available processor on the
system. We develop a lightweight simulator\cite{ocst} and an accompanying
multi-armed bandit library\cite{obandit}. See the reproducibility paragraph
below for more information.  With this simulator, we are able to replay
EASY-FCFS on the CEA-CURIE trace in 33 seconds with a machine equipped with an
Intel(R) Core(TM) i5-4310U CPU @ 2.00GHz. As a point of comparison, the Batsim
simulator with no network communication modeling and no resource contiguity
takes more than 7 hours to replay the same trace on the same hardware. The complete
simulation campaign presented below is executed on a single Dell PowerEdge R730 in 22
hours, including input preparation and analysis code.

\subsubsection{Resampling}
\label{ssub:resampling}

%Validating algorithms on real data is a notoriously hard
%problem~\cite{feitbook}. The case of batch scheduling is made especially
%difficult by the fluctuations in commonly used metrics~\cite{jsm}. Indeed, the
%variability in performance is known to outrank the available sample sizes:
%\textit{It is not enough to simply replay an algorithm using a trace}. For this
%reason, we clearly expose our statistical approach.

%Two main methods are available in order to generate data. On one hand, it is
%possible to model~\cite{feitperfeval} workloads. However, this often loses
%many specificities of the original data, and this work uses a second
%approach based on trace resampling~\cite{feitresampling}.

To carry our experiments, we need to generate many trace samples. More precisely, we want
to sample the job submission process, which is achieved by splitting the traces in weeks,
randomly shuffling them and picking the jobs submitted in the selected weeks.  This simple
mechanism enables us to preserve the dependency within a week. Although the system state
may not be independent from one week to an other, we can reasonably assume that it evolves
under its stationary distribution. Thus, computing an average over the sampled traces at a
given time can be related to a time average of an underlying Markov process, which
converges to the true quantity thanks to the ergodic theorem.  Moreover, the job
submission process has no long range correlation making it look like an independent and
identically distributed process when considering sufficiently time spaced weeks.

\subsubsection{Experimental campaign}


\subsubsection{Replicability}

Readers interested in replicating the experiments (or a part thereof) are
invited to peruse to the 'artifacts description' appendix of this paper.  We
provide to persistent archives containing the simulator used, the analysis code
and the associated workflow necessary to replicate all experiments and figures
present in this paper. All these parts's dependencies are automatically managed
via Nix~\cite{nix} and we invite the reader to replicate the results.

\subsection{Results}
\label{sub:results}

\begin{figure*}[h]
  \centering
  \includegraphics[scale=0.6]{figures/CTC-SP2.pdf}
  \includegraphics[scale=0.6]{figures/KTH-SP2.pdf}\\
  \includegraphics[scale=0.6]{figures/CEA-Curi.pdf}
  \includegraphics[scale=0.6]{figures/SDSC-BLU.pdf}\\
  \includegraphics[scale=0.6]{figures/SDSC-SP2.pdf}
  \includegraphics[scale=0.6]{figures/ANL-Intr.pdf}

  \caption{Evolution of the average cumulative waiting time improvement
    compared to EASY-FCFS of the FullFeedback, NoisyFeedback and EpsilonGreedy
    policies. The average is obtained by resampling the original trace 100
    times. The dashed lines represent the 10th and 90th percentiles of the
  values across this resampling. Each figure is a different trace, and this
figure is followed-up in figure~\ref{fig:follow} for the UniLu-Gaia log.}

  \label{fig:small}
\end{figure*}

\begin{figure}[ht]
  \centering
  \includegraphics[scale=0.6]{figures/UniLu-Ga.pdf}

  \caption{Follow-up from figure~\ref{fig:small}, plot for the UniLu-Gaia log.}

  \label{fig:follow}
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[scale=0.6]{figures/full-UniLu-Ga.pdf}

  \caption{Evolution of the average cumulative waiting time improvement
    compared to EASY-FCFS of the policies $P_i$ for $i = 1 \ldots 10$ on the
    UniLu-Gaia trace. The
    average is obtained by resampling the original trace 100 times. The dashed
    lines represent the 10th and 90th percentiles of the values across this
  resampling.}

  \label{fig:all}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.6]{figures/mosaicbandit-UniLu-Ga.pdf}
  \caption{Share of the policies chosen by Epsilon-Greedy as a function of time.
  The average choice is obtained by resampling the original trace 100 times and
  aggregating by date.}
  \label{fig:mosb}
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[scale=0.6]{figures/mosaic-UniLu-Ga.pdf}
  \caption{Share of the policies chosen by the Noisy policy. The average
  choice is obtained by resampling the original trace 100 times and
  aggregating by date.}
  \label{fig:mosn}
\end{figure}

\section{Conclusion}
\label{sec:ccl}

\begin{acks}

Authors are sorted in alphabetical order. We warmly thank Pierre Neyron for help with the experiments. We gracefully thank the contributors
of the Parallel Workloads Archive, Victor Hazlewood (SDSC SP2), Travis Earheart
and Nancy Wilkins-Diehr (SDSC Blue), Lars Malinowsky (KTH SP2), Dan Dwyer and
Steve Hotovy (CTC SP2), Joseph Emeras (CEA Curie and UniLu Gaia), Susan
Coghlan, Narayan Desay, Wei Tang (ANL Intrepid), and of course Dror Feitelson.
The Metacentrum workload log was graciously provided by the Czech National Grid
Infrastructure MetaCentrum. This work has been partially supported by the LabEx
PERSYVAL-Lab(\grantsponsor{ANR-11-LABX-0025-01}{ANR-11-LABX-0025-01}) funded by
the French program Investissement d'avenir. Experiments presented in this paper
were carried out using the Grid'5000 testbed. Grid'5000 is supported by a
scientific interest group hosted by Inria and including CNRS, RENATER and
several Universities as well as other
organizations\footnote{https://www.grid5000.fr}.  Access to the experimental
machine(s) used in this paper was gracefully granted by research teams from
LIG\footnote{http://www.liglab.fr} and Inria\footnote{http://www.inria.fr}.

\end{acks}

\bibliographystyle{acmst}
\bibliography{bibliography}

\clearpage
\section*{Artifacts Description}

In this annex, we describe the computational artifacts associated with this
work. We take a lightweight approach to replicability by packaging a workflow
that generates all the figures from this paper and making it available
publicly. Moreover, questions regarding replication can be freely directed at
the authors via e-mail.

All the code associated with this work is released under the ~\cite{ISC}
license as a persistent Zenodo~\cite{zenodo} archive at~\cite{zenodomain}.

\subsection{Packaging and Replicability}

There are various approaches for making computational experiments replicable,
among which distributing complete operating system images, containers, or
packaging software. As our experiments are CPU bound, we decide to opt for the
software packaging approach using the Nix~\cite{nix} packaging system.  All the
dependencies of our experiments (including our own code, dependencies for data
processing and visualization and workflow engine) are automatically managed by
Nix up to the actual execution of the code.

Running the experiments can be done on any platform equipped with the nix
workflow system by running:

\begin{lstlisting}
nix-build (fetchTarball ...) -A banditSelection
\end{lstlisting}

\subsection{Testbed}

 The experiments in this paper can run via in less than 24 hours on a large
 machine. More precisely, we had access to a Dell PowerEdge R730 equipped with
 a total of 56 threads @2.4GHz each and 757G of RAM.  For this reason, the
 experimental campaign was designed to be executed on a single host.

 \subsection{Workflow}

 All experiments are tied together using Zymake~\cite{zymake}, a minimalistic
 workflow system designed for computational experiments. This system is
 analogous to a traditional build system with added workflow capabilities. The
 entire workflow that generates this article from the input data is contained
 in a single \lstinline[basicstyle=\ttfamily\color{blue}]|zymakefile| to be
 found at the root of the main archive. This workfloww is composed of the
 following steps:

 \begin{itemize}

   \item[Preprocessing]: Data extraction from archives, filtering, resampling.
     This is principally using shell scripts for data manipulation (see file
     \lstinline[basicstyle=\ttfamily\color{blue}]|misc/strong\_filter| for the
     filtering steps used) and ocaml code that implements the resampling method
     described in~\ref{sub:resampling}.

   \item[Simulation]: This step runs the lightweight ocaml backfilling
     simulator specially written for this work. This simulator is made
     available under the ISC~\cite{ISC} license both as a persistent Zenodo
     archive at~\cite{ocst} and as a git repository at ~\cite{ocstgit}.

   \item[Analysis]: This step runs mostly R code that generates the figures
     presented in this paper.
 \end{itemize}
\end{document}
