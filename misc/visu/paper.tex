\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}

\title{Insights in tuning EASY-Backfilling}

\author{
\IEEEauthorblockN{J\'er\^ome Lelong, Valentin Reis, Denis Trystram}
\IEEEauthorblockA{Grenoble Alpes University \\ firstname.lastname@imag.fr}
}

\author{J\'er\^ome Lelong \and Valentin Reis \and Denis Trystram}

\institute{Grenoble Alpes University, Grenoble, France \\ \email{firstname.lastname@imag.fr}}

\maketitle

\begin{abstract}

  EASY backfilling is a popular scheduling heuristic for allocating jobs in
  large scale High Performance Computing platforms. While its aggressive
  reservation mechanism is fast to execute and prevents job starvation, it
  remains largely suboptimal with respect to any scheduling objective. We
  consider the problem of tuning EASY using job reordering policies. More
  precisely, we propose a methodology based on the use of a simulator to choose
  the best heuristic for a given system, so that a cumulative scheduling
  objective such as the bounded slowdown is minimized on a yet unseen workload.
  This methodology introduces a risk on the maximum values of typical objective
  values, which are controlled in this paper by a queue thresholding mechanism.
  This new approach is evaluated through a comprehensive experimental campaign
  on several production logs. We show in particular that the performance does
  generalize on all systems under study. Indeed, the average bounded slowdown
  can be reduced between 86\% to 52\% of the its value for EASY on various
  workload logs from the Parallel Workload Archive.  This work studies
  empirical performance generalization of reordering policies in a robust
  framework and demonstrates that effort should be put on learning to reorder
  job queues.

\end{abstract}

\section{Introduction}

High Performance Computing platforms are evolving to extreme scale multi-cores.
Such systems provide the capabilities needed to solve large scaie complex
scientific, economic or social problems.  The number of processors will
drastically increase and more processing capabilities will obviously lead to
more data produced~\cite{DOEASCAC}.  Moreover, new computing systems are
expected to run more flexible workloads.  Seldom supported by the existing
managing resource systems, the future schedulers should take advantage of this
flexibility to optimize the performance of the system.  The extreme scale
generates a huge amount of data at run-time. Collecting relevant information is
a prerequisite for determining efficient allocations.

The resources of such platforms are usually the subject of competition by
many users submitting their jobs, and parallel job scheduling is the key
problem to solve. Efficient scheduling of parallel jobs is a challenging task
which promises great improvement in various ways, including improved machine
utilization, energy efficiency, throughput and response time. Not only are
scheduling problems usually computationally hard, but in HPC practice they are
also plagued with uncertainty as many parameters of the problem are unknown
while taking decisions. As a consequence, the actual production platforms
currently rely on very simple heuristics based on queues of submitted jobs,
ordered in various ways.  The most used heuristic is the well-known
EASY-backfilling policy~\cite{easy}. While EASY is simple, fast to execute and
prevents starvation, it does not fare especially well with respect to
cumulative cost metrics\footnote{These metrics are defined in section
\ref{sec:pbset}} such as the average Bounded Slowdown(\textbf{AvgBsld}) or the
average waiting time (\textbf{AvgWait}) of jobs.  Therefore, many HPC
researchers and system administrators intent to tune this heuristic by
reordering the \textit{job starting/reservation} and \textit{backfilling}
queues. Since such reordering of job queues may introduce starvation in the
scheduling, this results in a dilemna between expected and maximal cost. In
order to solve this dilemna, we introduce a thresholding mechanism that can
effectively manage the risk of reaching maximum objective values. This issue is
further complexified by the dependence of the relative scheduling performances
on system characteristics and workload profiles. We propose in this work the
use of simulations to select queue reordering policies.  Finally, we study the
expirical generalization and stability of this methodology and open doors to
further learning-based approaches.

Section~\ref{sec:pbset} describes the context and details the problem
statement. Section~\ref{sec:rw} reviews existing approaches from the
literature. Sections~\ref{sec:mmax} and~\ref{sec:th} will introduce our
approach , illustrating the discussion with results from the KTH-SP2 trace.
Section~\ref{sec:expe} validates the approach using a comprehensive
experimental campaign on 7 logs from the Parallel Workload
Archive\cite{Feitelson20142967}.

\section{Related Works}
\label{sec:rw}

While parallel job scheduling is a well studied theoretical
problem~\cite{leung2004handbook}, the practical ramifications, varying
hypotheses, and inherent uncertainty of the problem in HPC have driven
practicioners and researchers alike to use and study simple heuristics.
The two most used heuristics for HPC systems are EASY~\cite{easy} and
Conservative~\cite{Mu'alem:2001:UPW:380314.380315} Backfilling.

While Conservative Backfilling offers many
advantages~\cite{srinivasan2002characterization}, it has a computational
overhead, perhaps explaining why most of the machines from the
top500~\cite{top500} ranking still use at the time of publication a variant of
EASY Backfilling.

There is a large body of work that seeks to improve EASY. Indeed, while the
heuristic is used by various resource and job management software (most notably
SLURM~\cite{SLURMdocSCHED}), this is rarely done without fine tuning by systems
administrators.

There are multiple works that explore how to tune EASY by reordering waiting
and/or backfilling queues\cite{Tsafrir_easypp_2005}, sometimes even in a
randomized manner~\cite{1592720}, as well as some
implementations~\cite{Jackson2001}. However successful they may be, these works
do not address the dependency~\cite{variability} of scheduling metrics on the
workload. Indeed, these studies often report \textit{post-hoc} performance.

The dynP~\cite{streit_selftuning_2002} scheduler does propose a systematic
method to tuning these queues, although it requires simulated scheduling runs
at decision time and therefore does not enjoy the natural speed of execution
of EASY.

There is a recent focus on leveraging the high amount of data available in
large scale computing systems in order to improve their behavior. Some works
use collaborative filtering to colocate tasks in clouds by estimating
application interference~\cite{7516031}. Others~\cite{fmodeling} are closer to
the application level and use binary classification to distinguish benign
memory faults from application errors in order to execute recovery algorithms.

A few works~\cite{Tsafrir_easypp_2005,learningruntimes} use this method in HPC,
hoping that better job runtime estimation should improve scheduling
~\cite{chiang_impact_2002}. Some algorithms estimate runtime
distributions models and choose jobs using probabilistic integration
procedures~\cite{Nissimov2008}.

However, these works do not address the duality between the cumulative and
maximal scheduling costs, as previously outlined in Figure~\ref{tab:maxbsldlit} for
the EASY++~\cite{Tsafrir_easypp_2005} and Easy-Eloss~\cite{learningruntimes}
heuristics.

While these works try to estimate uncertain parameters, this paper takes a more
basic approach and directly tries to learn a good scheduling policy from a
simple policy space.

\begin{figure*}[ht!]
  \centering
  \includegraphics[scale=0.5]{\paretobasicpdf}

  \caption{Performance of the 49 Backfilling heuristics generated by using the
    7 possible policies as queue and backfilling order, averaged over 2000
    resampled weeks from the \textit{training} KTH-SP2 trace, in terms of both
    maximum and average waiting time of the jobs. All values are normalized by
  that of \textbf{FCFS} reservations with \textbf{FCFS} backfilling and the
error bars correspond to the standard deviation.}

  \label{fig:paretobasic}
\end{figure*}

\section{Experimental Protocol}
\label{sec:expe}

This section motivates the statistical approach used to measure performance,
describes the simulation method used and explains the train/test protocol used
to evaluate the generalization ability of heuristics. Finally, it describes the
workload logs and concludes on the results of the experimental campaign.

\subsection{Statistical approach}
\label{sub:sa}

\begin{figure*}[ht!]
  \centering
  \includegraphics[scale=0.5]{\variabilitypdf}
  \includegraphics[scale=0.5]{\variabilitynormpdf}

  \caption{AvgWait cost of using the 7 main queue policies with \textbf{FCFS}
    backfilling for 100 generated weeks. First, in absolute value, and then
    normalized on the cost of using the \textbf{FCFS} policy to order the
  reservation queue (in yellow). }

  \label{fig:variability}
\end{figure*}

The experimental approach used in this paper is statistical in nature.
Figure~\ref{fig:variability} shows how the AvgWait costs of the 7 reservation
policies used along with FCFS backfilling evolves troughout the first 30 weeks
of the original KTH-SP2 trace. The
variability~\cite{variability,frachtenberg2005pitfalls} of cost metrics and
their sensitivity to small changes in the workload logs~\cite{flurries} have been
thouroughly studied in the literature. Our approach to measuring performance
without reporting noise from workload flurries~\cite{flurries} is to aggregate
the cost metric on a large number of generated logs. The approach of this
paper follows in part \cite{feitresampling} and design a trace resampler in
order to generate week-long workload logs from an original dataset. The
resampling technique used is simplistic in nature: for each system user, a
random week of job submissions from the original trace is used.  This approach
is combinatorially sufficient to generate a quasi-infinity of logs while
preserving the natural dependence of the workload on the weekly period and the
variability in load. On the downside, the seasonal effect and the dependence
between users is lost.  Moreover, there is no user model or other feedback loop
in the simulations. In all experiments, the performance of every policy is
evaluated by averaging the cost values over 2000 generated weeks.

\subsection{Simulation method and testbed}

While high quality simulators like SimGrid~\cite{casanova:hal-01017319} are
available in practice, this paper focuses on backfilling behavior and does not
use such advanced models. This is motivated by the fact that one needs to use a
high-performance approach to simulation in order to perform the high number of
scheduling runs necessary for this study (The total number of week-long
simulations in this paper reaches X). Therefore, experiments are run with a
specially designed backfilling scheduler that trades faithfulness for speed by
not including locality. Using this simulator, a week of EASY-fcfs backfilling
can be replayed in under 10'', I/O operations (reading and writing a swf file)
included. All simulations are performed on a Dell PowerEdge T630 machine with
2x Intel(R) Xeon(R) CPU E5-2697 v3 @ 2.60GHz/14 cores (28 cores/node), and 260
GB of RAM. We use a minimalistic aprroach to reproducible
research~\cite{stodden2014implementing} and provide a snapshot of the work that
includes a build system that runs the experiments using the
\textit{zymake}~\cite{breck2008zymake} minimalistic workflow system.  The
archive includes our simulator and a nix~\cite{dolstra2004imposing} file that
describes the dependencies.

\subsection{Generalization protocol}
\label{sub:gp}

The goal of this paper to study how the performance of different heuristics
generalize. The protocol is as follows: The initial workload is split
at temporal midpoint in two parts, the \textit{training} and \textit{testing}
logs. Each of these are used to resample weeks. Morally, this corresponds to
the situation where a systems administrator having retained usage logs from a
HPC center must choose a scheduling policy for the next period using
simulation. For each HPC log from the Parallel Workload archive used in the
experiment, this process results in two databases of 2000 weeks each.

\section{Problem Setting}
\label{sec:pbset}

This section describes a generic platform model used in this paper, presents the EASY
heuristic and defines two scheduling cost metrics to be minimized. Finally, it
motivates and introduces the problem statement of this paper.

\subsection{System Description}
\label{sub:sysdesc}

The problem adressed in this paper is the one faced by Resource and Job
Management Systems (RJMS) such as SLURM\cite{SLURMdocSCHED}, PBS\cite{PBSdoc}
and OAR\cite{capit2005batch}.

The crucial part of these software is the scheduling algorithm that determines
where and when the submitted jobs are executed. The process is as follows: Jobs
are submitted by end-users into queues until the scheduler selects one of them
for running. Each job has an provided bound on the execution time and some
resource requirements (number and type of processing units). Then, the RJMS
drives the search for the resources required to execute this job.  Finally, the
tasks of the job are assigned to the chosen nodes.

In the classical case, these software need to execute a set of concurrent
parallel jobs with rigid (known and fixed) resource requirements on a HPC
platform represented by a pool of $m$ identical resources. This is an online
scheduling problem as jobs are submitted over time.

A job $j$ has the following characteristics:

\begin{itemize}
  \item Submission date $r_j$ (sometimes called \textit{release date})

  \item Resource requirement $q_j$ (processor count)

  \item Actual running time  $p_j$ (sometimes called \textit{processing time})

  \item Requested running time $\widetilde{p_j}$ (sometimes called \textit{walltime}), which is an upper bound on $p_j$
\end{itemize}

The resource requirement $q_j$ of a job is known when the job is submitted at
time $r_j$, while the requested running time $\widetilde{p_j}$ is given by the
user as an estimate. Its actual value $p_j$ is only known \textit{a posteriori}
when the job really completes.  Moreover, the user has incentive to
overshoot the actual value, since his job could get ``killed'' if it
surpasses the provided value.

\subsection{EASY Backfilling}
\label{sub:notation}

The selection of the job to run is performed according to a
scheduling policy that establishes the order in which the jobs are executed.
EASY-Backfilling is the most widely used policy due to its easy and robust
implementation and known benefits such as high system
utilization~\cite{Feitelson}. This strategy has no worst case guarantee beyond
the absence of starvation (i.e. every job will be scheduled at some moment).

The EASY heuristic uses a job queue to perform job
\textit{starting/reservation} and job \textit{backfilling}. These queues can be
dissociated and the heuristic can be parametrized via both a reservation policy
and a backfilling policy. This is typically done by ordering both queues in an
identical manner using job attributes. In the following, we denote by
EASY-$P_R$-$P_B$ the scheduling policy that starts jobs and does the
reservation according to policy $P_R$ and backfills according to policy $P_B$.
For the sake of completeness, Algorithm~\ref{alg:EASY} describes the
EASY-$P_R$-$P_B$ heuristic.

\begin{algorithm}[H]
  \caption{EASY-$P_R$-$P_B$ policy}
  \begin{algorithmic}[1]
    \renewcommand{\algorithmicrequire}{\textbf{Input:}}
    \renewcommand{\algorithmicensure}{\textbf{Output:}}
    \REQUIRE Queue $Q$ of waiting jobs.
    \ENSURE None (calls to $Start()$)
    \\ \textit{Starting jobs in the $P_R$ order}
    \STATE Sort $Q$ according to $P_R$
    \FOR {job $j$}
    \STATE Pop $j$ from Q
    \IF {$j$ can be started given current system use.}
    \STATE $Start(j)$
    \ELSE
    \STATE Reserve $j$ at the earliest
    time possible according to the estimated running times
    of the currently running jobs.
    \\ \textit{Backfill jobs in the $P_B$ order}
    \STATE $L \leftarrow Q$
    \STATE Sort $L$ according to $P_B$
    \FOR {job $j'$ in $L$}
    \IF {$j'$ can be started without delaying the reservation on $j$.}
    \STATE $Start(j')$
    \ENDIF
    \ENDFOR
    \STATE \textbf{break}
    \ENDIF
    \ENDFOR
  \end{algorithmic}
  \label{alg:EASY}
\end{algorithm}

This paper makes use of 7 simple queue reordering policies, some of which are
often studied in the literature:


\begin{itemize}
  \item FCFS: First-Come First-Serve, the widely used default~\cite{easy}.
  \item LCFS: Last-Come First-Serve.
  \item LPF: Longest estimated Processing time $\widetilde{p_{j}}$ First.
  \item SPF: Smallest estimated Processing time $\widetilde{p_{j}}$ First ~\cite{bfchar}.
  \item LQF: Largest resource requirement $q_j$ First.
  \item SQF: Smallest resource requirement $q_j$ First.

  \item EXP: Largest Expansion Factor~\cite{bfchar} First, where the expansion
    factor is, by writing $start_j$ the starting time of a job:

  \begin{equation} \frac{start_j - r_j + \widetilde{p_j}}{\widetilde{p_j}} \end{equation}
\end{itemize}

\subsection{Scheduling Costs}
\label{sub:scheduling_objectives}

A system administrator may use one or multiple cost metric(s). Our study of
scheduling performance relies on two of the more frequently used:

\begin{itemize}
  \item The Waiting Time of a job:
    \begin{equation}
      \textbf{Wait}_j =  start_j-r_j
    \end{equation}
  \item The Average Bounded Slowdown~\cite{frachtenberg2005pitfalls,feitelson1998metrics} , a more refined metric:
    \begin{equation}
      \textbf{Bsld}_j = \max \left(  1,\frac{\textbf{Wait}_j + p_j}{\max \left( \tau,p_j \right) } \right)
    \end{equation}
    where $\tau$ is set as is usual~\cite{feitelson1998metrics} to 10 seconds.
\end{itemize}

Both of these cost metrics are usually considered in their \textit{cumulative}
version, which means that one seeks to minimize the Average Bounded Slowdown
(\textbf{AvgBsld}) or the (\textbf{AvgWait}). In the following, we will also
use the maximal version of these costs, which we denote by \textbf{MaxWait} and
\textbf{MaxBsld}, a.k.a the maximal value of the waiting time and bounded
slowdown on all the jobs from a scheduling run.

\subsection{Problem Description}
\label{sec:minmax}

There are in the author's view two main difficulties when efficiently tuning the
EASY heuristic. Each of these two issues are illustrated here by a small
scheduling experiment.

First, the relative performance of EASY policies is sensitive to
context~\cite{variability,bfchar}. Table~\ref{tab:context} illustrates this
effect by comparing the AvgBsld of two different queue ordering policies on
logs two different workloads. This hints to the fact that there is no "one size
fits all" choice of reservation and backfilling policies.  In such a situation,
tuning of the EASY heuristic must be done locally for each HPC system. This can
be done via simulation, in which case one is interested in how the simulation
results \textit{generalize} to the future.

\input{\filetablec}

\begin{table*}[ht!]
  \centering
  \ra{1.3}

  \caption{\textbf{AvgBsld} and \textbf{MaxBsld} of two "statistical" EASY
  Policies versus EASY-FCFS-FCFS. This is a single-run experiment on the
initial workload logs described in Section\ref{sec:expe}.}

  \label{tab:maxbsldlit}
  \begin{tabular}{lrrrrrr}
    \hline
                                      & KTH-SP2 & CEA-CURIE & CTC-SP2 & SDSC-BLUE & SDSC-SP2 \\
    \hline
    \textbf{AvgBsld} cost :           &         &           &         &           &          \\
    \hline
    EASY-FCFS                         & 92.5    & 202.0     & 49.5    & 36.5      & 87.8     \\
    EASY-Eloss\cite{learningruntimes} & 51.4    & 27.8      & 20.5    & 12.9      & 74.9     \\
    EASY++\cite{Tsafrir_easypp_2005}  & 63.5    & 193.0     & 85.7    & 20.9      & 79.3     \\
    \hline
    \textbf{MaxBsld} cost :           &         &           &         &           &          \\
    \hline
    EASY-FCFS                         & 14805   & 9971      & 8137    & 3395      & 5981     \\
    EASY-Eloss                        & 39929   & 511197    & 52933   & 21750     & 190656   \\
    EASY++                            & 30250   & 286588    & 66691   & 15530     & 13681    \\
    \hline
  \end{tabular}
\end{table*}

Second, starvation may appear when changing the EASY queue policy away
from FCFS. This issue concerns the method used to measure the objective. Most
systems use a variant of the FCFS-FCFS policy, where the FCFS policy is used
both for job starting and reservation and for backfilling jobs. The main
advantage of this choice is that it controls the \textit{starvation risk} by
greedily minimizing the maximum values of the job waiting times.
Indeed, a job might be indefinitely delayed when not starting jobs in the FCFS
order. Table~\ref{tab:maxbsldlit} illustrates how this effect appears in
several related works \cite{Tsafrir_easypp_2005,learningruntimes} that optimize
the average cost by removing the FCFS constraint.

In this paper, we ask the following: \textbf{How to leverage workload data in
order to improve cumulative cost metrics while controlling their maximum
values?}

In order to answer this question, we investigate the use of simulation to tune
EASY Backfilling by reordering its two queues.  The first conclusion is that
reordering the job starting and reservation queue is more beneficial than
simply reordering the backfilling queue. However, this introduces a risk on the
maximum values of the objective, which we control by hybridizing FCFS and the
reordering policy via a thresholding mechanism.  Finally, we show that the
experimental performance of the thresholded heuristics generalizes well.

\section{Maximum, Average cost and Experimental Generalization}
\label{sec:mmax}


\begin{figure*}[ht!]
  \centering
  \includegraphics[scale=0.5]{\paretopdf}

  \caption{Performance of the 49 Backfilling heuristics generated by using the
    7 possible policies as queue and backfilling order, averaged over 300
    resampled weeks, in terms of both maximum and average waiting time of the
    jobs. The transparent triangular points correspond to the \textit{training}
    trace and the hard round points to the \textit{testing} trace. All
    \textit{training} and \textit{testing} values are normalized with respect
    to the \textbf{EASY} reservations with \textbf{EASY} backfilling.}

  \label{fig:paretogen}
\end{figure*}

This section presents a small experimental campaign that uses the KTH-SP2 trace
in order to illustrate the contradictory effect of averaged and maximum cost.
In order to report results accurately, all results presented from this point
use the experimental protocol described in Section~\ref{sec:expe}.  The reader
is invited to skim on subsections~\ref{sub:sa} and~\ref{sub:gp} before going
further as it will improve the understanding of the following paragraphs.
Figure~\ref{fig:paretobasic} shows how the cumulative and maximal costs of all
policy couples (one policy for the reservation order and one for the
backfilling order) evolve when various policies are used for reservation and
backfilling. There are two main observations.

First, It seems possible to improve the AvgWait on this machine as far
as to divide it by two compared to the EASY FCFS-FCFS baseline. However, such
AvgWait improvements seem to entail an increase in MaxWait
cost. Expectedly, the EASY-FCFS-FCFS heuristic has a good MaxWait behavior.

Second, there seem to be regularities in the behavior of the performance: The
main factor seems to be the reservation policy, while the importance of the
backfilling policy varies depending on the reservation policy used. It appears
that some policies such as SQF do not lead to many backfilling decisions, while
others like LQF encourage frequent backfilling. Additionaly, there are some
backfilling policies, such as SPF and ExpFact that seem to consistently
outperform the others.

The traditional approach to multi-criteria optimization is to choose a point
using the pareto frontier. However, one must keep in mind that these results
are obtained \textit{in hindsight} on the \textit{training} trace and do not
necessarily generalize well to the future.

\subsection{Generalization of average and maximum cost}

This section discusses the generalization of cost metrics in terms of average
and maximum values. The point is illustrated as in the previous section by
using the KTH-SP2 trace and the experimental protocol described in section
\ref{sec:expe}.


Figure~\ref{fig:paretogen} shows the evolution of the AvgWait and MaxWait costs
obtained by the 49 heuristics between \textit{training} and \textit{testing}
logs. Training (transparent) and testing (hard) values are connected by
dashed lines. The relative performance of queue policies seems to be somehow
stable. However, there is a displacement of values due to changes in the
characteristics of the second part of the original trace\footnote{An apparent
  difference between training and testing parts of the KTH-SP2 trace is the
decrease in load.}.

Since the ordinal performances seem roughly conserved, one can hope to
generalize with respect to the cumulative cost. However, the variability in
values indicates that one can not use classic pareto curve methods to
generalize from training to testing logs in regions of low maximum cost. The
next section therefore introduces a thresholding mechanism in order to control
the maximum cost.

\begin{figure*}[ht!]
  \centering
  \includegraphics[scale=0.5]{\paretothpdf}

  \caption{Performance of the 49 Backfilling heuristics generated by using the
    7 possible policies as queue and backfilling order, averaged over 300
    resampled weeks from the \textit{training} trace, in terms of both maximum
    and average waiting time of the jobs. The transparent triangular points
    correspond to the policies without threshold and the hard round points to the
    thresholded version. All values are normalized with respect to the
    \textbf{EASY} reservations with \textbf{EASY} backfilling.}

\end{figure*}

\section{Thresholding}

This section introduces control over maximal costs using a simple thresholding
mechanism.

\subsection{Thresholding and risk}
\label{sec:th}

The future costs $\text{Wait}_j$ and $\text{Bsld}_j$ of a waiting job $j$
are lower-bounded at any time $t$ by the "waiting time so far" $t-r_j$. A
simple way to introduce robustness into the heuristic is therefore to force
jobs with unusually high values of $t-r_j$ ahead of the reservation queue. One
way to do this is to introduce a threshold parameter $T$ and push jobs with
$t-r_j>T$ immediately ahead of the reservation queue after the reservation
queue sorting step (line 1 of Algorithm~\ref{alg:EASY}). If more than one job
is in this situation, these jobs are ordered by submission time $r_j$ at the
head of the queue. Figure~\ref{fig:paretoth} illustrates the effect this has on
the 49 possible heuristics on the KTH-SP2 system with $T = 20$ hours. The
MaxWait cost is greatly reduced, while all AvgWait values are
(perhaps expectedly) moved torwards EASY-FCFS-FCFS. Note that while the
variance in terms of MaxWait is greatly reduced, the variance in terms
of AvgWait is not reduced as much. This is the effect this paper
leverages. The next subsection gives a glimpse of the behavior of
generalization in this framework.


\subsection{Thresholding and generalization}
\label{sec:genth}

The final step is to study how the performance thresholded queue policies
generalizes. Figure~\ref{fig:parallelkth} shows how the performance of all
various queue and backfilling policies evolve from \textit{training} to
\textit{testing} logs when the threshold is set to an example value of 20
hours. While values do change from \textit{training} to \textit{testing}
logs, the relative order of policies seems to be roughly conserved. This
leaves hope for generalization.

\begin{figure}[ht!]
  \centering
  \includegraphics[scale=0.5]{\kthparallelpdf}

  \caption{AvgBsld generalization of thresholded policies obtained by using a
    threshold value of 20 hours. Note that each plot has a different vertical
    axis. The reported AvgBsld and MaxBsld values are averaged over 300
    resampled weeks from the training or testing original logs, normalized over
    the cost of EASY-FCFS-FCFS. The "Worst MaxBsld" value is the highest (300
    week averaged) maximum average bounded slowdown increase percentage
    reported for any of the 49 heuristics. The values reported under Learned
    Performance are the (300 week averaged) decrease percentages obtained by the
    best \textbf{Training} heuristinc on the \textbf{Testing} logs.}

  \label{fig:parallelkth}
\end{figure}

The next section will outline the experimental setup and present the results
from a comprehensive experimental campaign that validates this effect.

\section{Experimental Validation}

\subsection{Workload logs}

Table~\ref{tab:logs} outlines the six workload logs from the Parallel
Workloads Archive~\cite{Feitelson20142967} used in the experiments. The logs
are subjected to pre-filtering. The filtering step excludes jobs with
$\widetilde{p_j} < p_j $ and jobs whose "requested\_cores" and "allocated\_cores"
fields exceed the size of the machine.

\begin{table}[h]
  \vspace{-0.5cm}
  \centering
  \ra{1.3}
  \caption{Workload logs used in the simulations.}
  \label{tab:logs}
  \begin{tabular}{@{}lrrrr@{}}
    \hline
    Name      & Year & \# CPUs      & \# Jobs       & Duration\\
    \hline
    KTH-SP2     & 1996 & 100       & 28k        & 11 Months\\
    CTC-SP2     & 1996 & 338       & 77k        & 11 Months\\
    SDSC-SP2    & 2000 & 128       & 59k        & 24 Months\\
    SDSC-BLUE   & 2003 & 1,152     & 243k       & 32 Months\\
    Curie       & 2012 & 80,640    & 312k       & 3  Months\\
    Metacentrum & 2013 & 3,356     & 495k       & 6  Months\\
    \hline
  \end{tabular}
  \vspace{-0.5cm}
\end{table}

\subsection{Results}

Figure~\ref{fig:threshold} (resp: Figure~\ref{fig:thresholdb}) summarizes the
behavior of Wait (resp: Bsld) generalization and risk with
respect to the value of the threshold $T$. There is a fortunate effect in that
the values from the lower parts of the graphs (the \textit{AvgWait} cost) seem
to decrease faster than values from the upper part (the \textit{MaxWait} cost),
which increase linearly with $T$.

By using an aggressive appproach (no threshold), the AvgWait can be reduced
until 80\% to 65\% compared to the EASY-FCFS-FCFS baseline. However, in that
case the values of the MaxWait can jump as high as 250\% that of the baseline.
The same phenomenon happens with the aggressive approach on the AvgBsld, where
we can attain a reduction of the value until 80\% to 50\%, but this entails an
increase of MaxBsld values up to 120\% times that of the baseline.

By using a conservative approach (thresholding at 20 hours), the AvgWait can be
reduced until 90\% to 70\% in expectation, while keeping the MaxWait increase
under 175\% of the baseline in all cases. We observe again the same phenomenon using the
bounded slowdown, where the AvgBsld can be reduced until 83\% to 60\% while
keeping the MaxBsld under 105\% of the baseline in all cases.

Figure~\ref{fig:par} (resp. Figure~\ref{fig:par}) shows how the AvgWait (resp.
AvgBsld) cost of the 49 combination of queue and backfilling policies evolve
from training to testing logs when we use this conservative threshold of 20
hours, with a higher sample size that was not premitted by the previous
experiment. This confirms the previous values and gives visual insight into the
stability of the performance.

Figure~\ref{fig:thresholdb} shows the same behavior for the AvgBsld
values.

\begin{figure*}[ht!]
  \centering
  \includegraphics[scale=0.4]{\kthprogress}
  \includegraphics[scale=0.4]{\sdscbprogress}
  \includegraphics[scale=0.4]{\sdscsprogress}
  \includegraphics[scale=0.4]{\ctcprogress}

  \caption{AvgWait and MaxWait generalization of thresholded policies as
  affected by the queue threshold. The Value reported as "train" is that of the
least costly heuristic among the 49 possible policy parametrizations averaged
on the \textit{training} logs. The Value reported as "test" is the averaged
cost of the same heuristic on the \textit{testing} logs.}

  \label{fig:threshold}
\end{figure*}

\begin{figure*}[ht!]
  \centering
  \includegraphics[scale=0.4]{\kthprogressb}
  \includegraphics[scale=0.4]{\sdscbprogressb}
  \includegraphics[scale=0.4]{\sdscsprogressb}
  \includegraphics[scale=0.4]{\ctcprogressb}

  \caption{AvgBsld and MaxBsld generalization of thresholded policies as
  affected by the queue threshold. The Value reported as "train" is that of the
least costly heuristic among the 49 possible policy parametrizations averaged
on the \textit{training} logs. The Value reported as "test" is the averaged
cost of the same heuristic on the \textit{testing} logs.}

  \label{fig:thresholdb}
\end{figure*}

\begin{figure*}[ht!]
  \centering
  \includegraphics[scale=0.5]{\sdspbparallelpdf}
  \includegraphics[scale=0.5]{\sdspsparallelpdf}
  \includegraphics[scale=0.5]{\ctcparallelpdf}
  \includegraphics[scale=0.5]{\kthparallelpdf}

  \caption{AvgWait generalization of thresholded policies obtained by using a
  threshold value of 20 hours. Note that each plot has a different vertical y axis. The
reported AvgWait and MaxWait values are averaged over 300 resampled weeks from
the training or testing original logs, normalized over the cost of
EASY-FCFS-FCFS. The "Worst MaxWait" value is the highest (300 week averaged)
maximum average bounded slowdown increase percentage reported for any of the 49
heuristics. The values reported under Learned Performance are the (300 week
averaged) decrease percentages obtained by the best \textbf{Training}
heuristinc on the \textbf{Testing} logs.}

  \label{fig:par}
\end{figure*}

\begin{figure*}[ht!]
  \centering
  \includegraphics[scale=0.5]{\sdspbparallelbpdf}
  \includegraphics[scale=0.5]{\sdspsparallelbpdf}
  \includegraphics[scale=0.5]{\ctcparallelbpdf}
  \includegraphics[scale=0.5]{\kthparallelbpdf}

  \caption{AvgBsld generalization of thresholded policies obtained by using a
    threshold value of 20 hours. Note that each plot has a different vertical y
    axis. The reported AvgBsld and MaxBsld values are averaged over 300
    resampled weeks from the training or testing original logs, normalized over
    the cost of EASY-FCFS-FCFS. The "Worst MaxBsld" value is the highest (300
    week averaged) maximum average bounded slowdown increase percentage
    reported for any of the 49 heuristics. The values reported under Learned
    Performance are the (300 week averaged) decrease percentages obtained by the
    best \textbf{Training} heuristinc on the \textbf{Testing} logs.}

  \label{fig:parb}
\end{figure*}

\section{Conclusion}

This work leverages the fact that the performance of scheduling heuristics
depend on the workload profile of the system. More precisely, we investigate
the use of simulation to tune EASY Backfilling by reordering its two queues.
The first conclusion is that reordering the job starting and reservation queue
is more beneficial than simply reordering the backfilling queue. However, this
introduces a risk on the maximum values of the objective, which we control by
hybridizing FCFS and the reordering policy via a thresholding mechanism.
Finally, we show that the experimental performance of the thresholded
heuristics generalizes well. Therefore, this framework allows a system
administrator to tune EASY using a simulator. Moreover, the attitude torwards
risk in maximum values can be specified via the threshold value. With a low
threshold value, the increase in maximal cost is small but the learned policy
does not take many risks. With a high threshold value, it's possible to gain
more, but this comes with an increase in the maximal cost. Two questions
concerning the learning of EASY policies arise from this work.

First, the stability of other EASY heuristic classes remains unknown. The
simple class of composed of 7 reservation policies and 7 backfilling policies
(cardinality 49) can generalize using thresholding. It is natural to ask
whether it could be possible to learn using a larger class of heuristics.  One
could for instance consider the class of mixed policies that choose a job based
on a linear combination of the 7 criteria. A more ambitious endeavor is to ask
whether it is possible to learn a contextual job ranking
model~\cite{joachims2002optimizing} that performs well.

\textit{ a retirer?
Second, the performance improvement achievable by the best reordering of
backfilling and reservation queues is unknown. Answering this hard optimization
task would give a lower bound on the achievable costs.
}

%\section{Annex}

%This section presents for the sake of completeness the equivalent of
%Figure~\ref{pareto} for the remaining workload logs.

\section{Acknowledgements}

Authors are listed in alphabetical order. We warmly thank Frederic Wagner and
Eric Gaussier for discussions as well as Pierre Neyron and Bruno Breznik for
their invaluable help with experiments. We gracefully thank the contributors of
the Parallel Workloads Archive, Victor Hazlewood (SDSC SP2), Travis Earheart
and Nancy Wilkins-Diehr (SDSC Blue), Lars Malinowsky (KTH SP2), Dan Dwyer and
Steve Hotovy (CTC SP2), Joseph Emeras (CEA Curie), and of course Dror
Feitelson.  The Metacentrum workload log was graciously provided by the Czech
National Grid Infrastructure MetaCentrum. This work has been partially
supported by the LabEx PERSYVAL-Lab (ANR-11-LABX-0025-01) funded by the French
program Investissement d'avenir. Experiments presented in this paper were
carried out using the Digitalis platform\footnote{http://digitalis.imag.fr} of
the Grid'5000 testbed. Grid'5000 is supported by a scientific interest group
hosted by Inria and including CNRS, RENATER and several Universities as well as
other organizations\footnote{https://www.grid5000.fr}. Access to the
experimental machine(s) used in this paper was gracefully granted by research
teams from LIG\footnote{http://www.liglab.fr} and
Inria\footnote{http://www.inria.fr}.

\bibliographystyle{o.splncs03}
\bibliography{o.bibliogr}

\end{document}
